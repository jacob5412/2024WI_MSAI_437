{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae4fef4f-4cb8-4937-8916-b989c4dbd668",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7674d4-8798-43d2-b231-6d9a7e5479ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "\n",
    "from models.multi_task_auto_encoder import MultiTaskAutoEncoder\n",
    "from training_testing.training_mtautoencoder import train_mtautoencoder\n",
    "from utils.data_loader import CustomMTImageDataset\n",
    "from utils.transformations_v2 import augment_dataset_with_replacement, resize_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a3dc0-09a3-4148-8a5b-23d694a6e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b616d7bd-cfcb-48bc-b727-fc35644f5adf",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511771c-a598-451e-a2ab-7148c6cb16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"valhalla/emoji-dataset\", cache_dir=\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d9918f-30ea-4539-9db2-8ffbe3ede645",
   "metadata": {},
   "source": [
    "### Creating a Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00877900-7b9f-446f-9ea6-2c6862b489e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_categories = [\n",
    "    \"face\",\n",
    "    \"vampire\",\n",
    "    \"elf\",\n",
    "    \"mage\",\n",
    "    \"hero\",\n",
    "    \"villain\",\n",
    "    \"evil monkey\",\n",
    "    \"zombie\",\n",
    "    \"haircut\",\n",
    "    \"juggling\",\n",
    "]\n",
    "\n",
    "data_subset = dataset[\"train\"].filter(\n",
    "    lambda example: any(\n",
    "        category in example[\"text\"] for category in expression_categories\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02520d1c-eafb-4e45-9350-b49229806988",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images related to expression categories:\", len(data_subset))\n",
    "print(\"Subset example:\", data_subset[25][\"text\"])\n",
    "sample_image = data_subset[25][\"image\"]\n",
    "display(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac7c35c-c5dc-4f10-b44e-fbe09c837e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_emoji(example):\n",
    "    human_like = [\n",
    "        \"face\",\n",
    "        \"superhero\",\n",
    "        \"supervillain\",\n",
    "        \"mage\",\n",
    "        \"vampire\",\n",
    "        \"elf\",\n",
    "        \"zombie\",\n",
    "        \"man\",\n",
    "        \"woman\",\n",
    "    ]\n",
    "    animals_mythical = [\n",
    "        \"cat\",\n",
    "        \"dog\",\n",
    "        \"monkey\",\n",
    "        \"fox\",\n",
    "        \"lion\",\n",
    "        \"tiger\",\n",
    "        \"horse\",\n",
    "        \"unicorn\",\n",
    "        \"cow\",\n",
    "        \"pig\",\n",
    "        \"mouse\",\n",
    "        \"rabbit\",\n",
    "        \"bear\",\n",
    "        \"frog\",\n",
    "        \"dragon\",\n",
    "    ]\n",
    "\n",
    "    description = example[\"text\"]\n",
    "    if any(word in description for word in human_like):\n",
    "        return {\"class\": 0}  # Human and Human-like Characters\n",
    "    elif any(word in description for word in animals_mythical):\n",
    "        return {\"class\": 1}  # Animals and Mythical Creatures\n",
    "    else:\n",
    "        return {\"class\": 2}  # Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2872980-3d24-4fb4-b1a3-3c1f01285633",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = data_subset.map(categorize_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8c6f0-50dd-4d3d-a330-2fe19a1fc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352677a5-2fda-4415-8ce9-d2109ebaeaff",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "\n",
    "Dividing this subset into training, validation and test sets using a 60/20/20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e9cabf-3f15-4010-bdaa-4b2d27831163",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(data_subset)\n",
    "train_size = int(0.6 * total_size)\n",
    "val_size = int(0.2 * total_size)\n",
    "test_size = total_size - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb8b5c-670f-4974-9134-bce94873b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [item[\"image\"] for item in data_subset]\n",
    "text = [item[\"text\"] for item in data_subset]\n",
    "labels = [item[\"class\"] for item in data_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabe0fd-da5a-4d47-97db-1caf73d56d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels, test_size=test_size, stratify=labels, random_state=random_seed\n",
    ")\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    test_size=val_size,\n",
    "    stratify=train_labels,\n",
    "    random_state=random_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5d149-e5ce-4334-a415-05d9b600bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [\n",
    "    {\"image\": img, \"class\": label} for img, label in zip(train_images, train_labels)\n",
    "]\n",
    "val_dataset = [\n",
    "    {\"image\": img, \"class\": label} for img, label in zip(val_images, val_labels)\n",
    "]\n",
    "test_dataset = [\n",
    "    {\"image\": img, \"class\": label} for img, label in zip(test_images, test_labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa41a9-e276-4d39-b1ce-7a5f2de5701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Split Size:\\n----------\")\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce1485-0314-4343-add5-af0d2a60b099",
   "metadata": {},
   "source": [
    "### Augmenting to 600/200/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22ff55-716f-4fa9-aeae-c3f2230190dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomRotation(degrees=15),  # Random rotation up to 10 degrees\n",
    "        transforms.RandomHorizontalFlip(\n",
    "            p=0.5\n",
    "        ),  # Random horizontal flip with a probability of 0.5\n",
    "        transforms.RandomVerticalFlip(\n",
    "            p=0.5\n",
    "        ),  # Random vertical flip with a probability of 0.5\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
    "        ),  # Randomly Adjust brightness, contrast, saturation, and hue\n",
    "        transforms.RandomAffine(\n",
    "            degrees=5, translate=(0.1, 0.1)\n",
    "        ),  # Random affine transformation\n",
    "        transforms.RandomApply(\n",
    "            [transforms.GaussianBlur(kernel_size=3)], p=0.1\n",
    "        ),  # Random Gaussian blur\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0f42e-34c4-4895-9a8e-8b47f37721e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = augment_dataset_with_replacement(\n",
    "    train_dataset, 600, augmentation_transforms\n",
    ")\n",
    "val_aug = augment_dataset_with_replacement(val_dataset, 200, augmentation_transforms)\n",
    "test_aug = augment_dataset_with_replacement(test_dataset, 200, augmentation_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f8eb68-83f4-470e-a5fe-1bec810c8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Augmented train dataset size:\", len(train_aug))\n",
    "print(\"Augmented validation dataset size:\", len(val_aug))\n",
    "print(\"Augmented test dataset size:\", len(test_aug))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc49c1-dbdd-4a91-be7d-118b89b7d5d8",
   "metadata": {},
   "source": [
    "### Resizing to 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29bc3b-e36b-4234-975a-192f7aa99e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_transform = transforms.Resize((64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107a402-d2db-4b56-805f-023fb44fa7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug_resized = resize_dataset(train_aug, resize_transform)\n",
    "val_aug_resized = resize_dataset(val_aug, resize_transform)\n",
    "test_aug_resized = resize_dataset(test_aug, resize_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a1203-f098-4907-a912-e1f5469dbf79",
   "metadata": {},
   "source": [
    "### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec4022-611e-42ed-89aa-efb3e8a90d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Subset class:\", train_aug_resized[500][\"class\"])\n",
    "sample_image = train_aug_resized[500][\"image\"]\n",
    "display(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc494266-0a4c-4206-8f62-31de8828451f",
   "metadata": {},
   "source": [
    "### Tensor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f7ad6-855c-45a1-b843-5a0dafd898ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomMTImageDataset(train_aug_resized)\n",
    "val_dataset = CustomMTImageDataset(val_aug_resized)\n",
    "test_dataset = CustomMTImageDataset(test_aug_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c7edce-1ff3-48a7-9cea-44758745bff5",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Do not run the code below unless you want to perform an extensive grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad1054-b806-4493-a816-ede3a21ce9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "latent_sizes = [(32, 2048), (64, 4096), (128, 8192), (256, 16384)]\n",
    "learning_rates = [0.001, 0.0001]\n",
    "weight_decays = [1e-5, 1e-4]\n",
    "encoder_channel_options = [(16, 32), (32, 64)]\n",
    "kernel_sizes = [3]\n",
    "strides = [2]\n",
    "paddings = [1]\n",
    "lambda_classifications = [0.1, 0.5, 1.0]\n",
    "\n",
    "num_epochs = 350\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8b09b-3ecb-455f-adc6-5525b88ce1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc21595-c0f1-4b19-b8ca-94700bf7f5be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (\n",
    "    (latent_channels, flattened_size),\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    encoder_channels,\n",
    "    kernel_size,\n",
    "    stride,\n",
    "    padding,\n",
    "    lambda_classification,\n",
    ") in itertools.product(\n",
    "    latent_sizes,\n",
    "    learning_rates,\n",
    "    weight_decays,\n",
    "    encoder_channel_options,\n",
    "    kernel_sizes,\n",
    "    strides,\n",
    "    paddings,\n",
    "    lambda_classifications,\n",
    "):\n",
    "    model = MultiTaskAutoEncoder(\n",
    "        latent_size=latent_channels,\n",
    "        flattened_latent_size=flattened_size,\n",
    "        encoder_channels=encoder_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "        num_classes=3,\n",
    "    )\n",
    "\n",
    "    (\n",
    "        _,\n",
    "        train_mse_losses,\n",
    "        train_classification_losses,\n",
    "        train_classification_accuracies,\n",
    "        val_mse_losses,\n",
    "        val_classification_losses,\n",
    "        val_classification_accuracies,\n",
    "    ) = train_mtautoencoder(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        lambda_classification=lambda_classification,\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot for MSE loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_mse_losses, label=\"Train MSE Loss\")\n",
    "    plt.plot(val_mse_losses, label=\"Validation MSE Loss\")\n",
    "    plt.title(\"MSE Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot for Classification loss\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(train_classification_losses, label=\"Train Classification Loss\")\n",
    "    plt.plot(val_classification_losses, label=\"Validation Classification Loss\")\n",
    "    plt.title(\"Classification Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot for Classification Accuracy\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(train_classification_accuracies, label=\"Train Classification Accuracy\")\n",
    "    plt.plot(val_classification_accuracies, label=\"Validation Classification Accuracy\")\n",
    "    plt.title(\"Classification Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"LS: {latent_channels}, LR: {lr}, WD: {weight_decay}, EC: {encoder_channels}, KS: {kernel_size}, S: {stride}, P: {padding}, LC: {lambda_classification}\"\n",
    "    )\n",
    "    plt.savefig(\n",
    "        \"learning_curves/question_2/\"\n",
    "        + f\"combined_metrics_ls{latent_channels}_lr{lr}_wd{weight_decay}_ec{encoder_channels}_ks{kernel_size}_s{stride}_p{padding}_lc{lambda_classification}.png\"\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # Store results\n",
    "    results.append(\n",
    "        {\n",
    "            \"latent_size\": latent_channels,\n",
    "            \"learning_rate\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"encoder_channels\": encoder_channels,\n",
    "            \"kernel_size\": kernel_size,\n",
    "            \"stride\": stride,\n",
    "            \"padding\": padding,\n",
    "            \"lambda_classification\": lambda_classification,\n",
    "            \"final_train_mse_loss\": train_mse_losses[-1],\n",
    "            \"final_val_mse_loss\": val_mse_losses[-1],\n",
    "            \"final_train_classification_loss\": train_classification_losses[-1],\n",
    "            \"final_val_classification_loss\": val_classification_losses[-1],\n",
    "            \"final_train_classification_accuracy\": train_classification_accuracies[-1],\n",
    "            \"final_val_classification_accuracy\": val_classification_accuracies[-1],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eea3ff-036b-45f5-905f-5a1ef481625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5e842-3700-4dbf-aeea-a2e3391703d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results/question_2/hyperparam_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d151c-f1b8-4a82-86e1-4af8558cdcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by=\"final_val_mse_loss\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b748fd3-0438-4c0b-aaf2-026e1cded1a3",
   "metadata": {},
   "source": [
    "## Testing Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88e93a-49da-400c-a357-9e429eac8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f501b-8092-4c2e-8e0e-e7053d6414ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f7f0a-659b-4fd2-b957-ea66c10c4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 256\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "encoder_channels = (32, 64)\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "lambda_classification = 0.1\n",
    "flattened_latent_size = 16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab952e92-f470-451f-bbfc-dc77d00ab530",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskAutoEncoder(\n",
    "    latent_size=latent_size,\n",
    "    encoder_channels=encoder_channels,\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    "    flattened_latent_size=flattened_latent_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137b399-fab6-4583-8139-e42459028641",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    _,\n",
    "    train_mse_losses,\n",
    "    train_classification_losses,\n",
    "    train_classification_accuracies,\n",
    "    val_mse_losses,\n",
    "    val_classification_losses,\n",
    "    val_classification_accuracies,\n",
    ") = train_mtautoencoder(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    lambda_classification=lambda_classification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cb976-d266-410a-a5a9-3cc47fe5f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot for MSE loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_mse_losses, label=\"Train MSE Loss\")\n",
    "plt.plot(val_mse_losses, label=\"Validation MSE Loss\")\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Classification loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_classification_losses, label=\"Train Classification Loss\")\n",
    "plt.plot(val_classification_losses, label=\"Validation Classification Loss\")\n",
    "plt.title(\"Classification Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Classification Accuracy\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(train_classification_accuracies, label=\"Train Classification Accuracy\")\n",
    "plt.plot(val_classification_accuracies, label=\"Validation Classification Accuracy\")\n",
    "plt.title(\"Classification Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"LS: {latent_channels}, LR: {lr}, WD: {weight_decay}, EC: {encoder_channels}, KS: {kernel_size}, S: {stride}, P: {padding}, LC: {lambda_classification}\"\n",
    ")\n",
    "plt.savefig(\n",
    "    \"results/question_2/\"\n",
    "    + f\"combined_metrics_ls{latent_channels}_lr{lr}_wd{weight_decay}_ec{encoder_channels}_ks{kernel_size}_s{stride}_p{padding}_lc{lambda_classification}.png\"\n",
    ")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7971ddb8-4f37-4159-a45d-e55c17b9852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "criterion_mse = torch.nn.MSELoss()\n",
    "criterion_classification = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "total_test_mse_loss = 0.0\n",
    "total_test_classification_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images, labels = batch[\"image\"], batch[\"class\"]\n",
    "        reconstructed, classification_logits = model(images)\n",
    "\n",
    "        loss_mse = criterion_mse(reconstructed, images)\n",
    "        loss_classification = criterion_classification(classification_logits, labels)\n",
    "\n",
    "        total_test_mse_loss += loss_mse.item() * images.size(0)\n",
    "        total_test_classification_loss += loss_classification.item() * labels.size(0)\n",
    "\n",
    "        _, predicted = torch.max(classification_logits, 1)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "        total_test += labels.size(0)\n",
    "\n",
    "test_mse_loss = total_test_mse_loss / len(test_loader.dataset)\n",
    "test_classification_loss = total_test_classification_loss / len(test_loader.dataset)\n",
    "test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "print(f\"Test MSE Loss: {test_mse_loss}\")\n",
    "print(f\"Test Classification Loss: {test_classification_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0081d1b-8795-45d9-9b3e-f8ca12bf8f24",
   "metadata": {},
   "source": [
    "## Saving Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403ab1c-1ffb-40ed-8e02-0aa857c6b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "from torchsummary import summary\n",
    "\n",
    "print(summary(model, (3, 64, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97a5d8-213b-4295-baa9-77abccd3e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving weights\n",
    "torch.save(model.state_dict(), \"results/question_2/q2_model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02617e2-9ee2-49ce-b750-86b94e8a8276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent representation\n",
    "model.eval()\n",
    "latent_representations = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch[\"image\"]\n",
    "        latent = model.encoder(images)\n",
    "        latent_representations.append(latent.cpu().numpy())\n",
    "\n",
    "latent_representations = np.concatenate(latent_representations, axis=0)\n",
    "\n",
    "latent_representations_path = \"results/question_2/latent_representations.npy\"\n",
    "np.save(latent_representations_path, latent_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba1180c-166b-42fb-8a36-d4f9080cd178",
   "metadata": {},
   "source": [
    "## Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209ca39-429c-4b33-b35f-12ec2b31f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# take the first image\n",
    "sample_batch = next(iter(test_loader))\n",
    "images, labels = sample_batch[\"image\"], sample_batch[\"class\"]\n",
    "sample_image, true_label = images[0], labels[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed, classification_logits = model(sample_image.unsqueeze(0))\n",
    "    predicted_label = torch.argmax(classification_logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f42a0fc-0a61-4c8f-b70f-a84f1c1ba46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert image\n",
    "sample_image_np = sample_image.numpy().transpose(1, 2, 0)\n",
    "reconstructed_np = reconstructed.squeeze(0).numpy().transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2fcfa-2219-435d-967f-c8693485001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_image_np)\n",
    "plt.title(f\"Original Image (True Class: {true_label})\")\n",
    "\n",
    "# Reconstructed Image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(reconstructed_np)\n",
    "plt.title(f\"Reconstructed Image (Predicted Class: {predicted_label.item()})\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c53490e-86bf-4c05-a42a-db200ba4ff53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
