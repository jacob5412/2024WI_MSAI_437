{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b36bcf-92f6-4e00-858d-678572a4bda4",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7829a6-d9ad-4983-8e9e-92365094c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "from models.auto_encoder import AutoEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from torchvision import transforms\n",
    "from torchviz import make_dot\n",
    "from training_testing import train_autoencoder\n",
    "from utils.data_loader import CustomImageDataset\n",
    "from utils.transformations import augment_dataset_with_replacement, resize_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6787c8a-0a5f-4504-9cd0-635cee7f0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3e567-839e-4b8c-9367-193fc9aea2cc",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086fc1f-e065-47f7-81ea-268b439d57f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"valhalla/emoji-dataset\", cache_dir=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a708cf41-5634-4f5a-a5ea-a8cd52b90a16",
   "metadata": {},
   "source": [
    "### Creating a Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e46c7-bfd2-43a9-b893-ab2f0c25cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_categories = [\n",
    "    \"face\",\n",
    "    \"vampire\",\n",
    "    \"elf\",\n",
    "    \"mage\",\n",
    "    \"hero\",\n",
    "    \"villain\",\n",
    "    \"evil monkey\",\n",
    "    \"zombie\",\n",
    "    \"haircut\",\n",
    "    \"juggling\",\n",
    "]\n",
    "\n",
    "data_subset = dataset[\"train\"].filter(\n",
    "    lambda example: any(\n",
    "        category in example[\"text\"] for category in expression_categories\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6118c6c-76d8-4bdb-a8d0-58a015d10a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images related to expression categories:\", len(data_subset))\n",
    "print(\"Subset example:\", data_subset[25][\"text\"])\n",
    "sample_image = data_subset[25][\"image\"]\n",
    "display(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969b162-8a67-47fa-a1d2-729a806fff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_width = 0\n",
    "max_height = 0\n",
    "\n",
    "for item in data_subset:\n",
    "    img = item[\"image\"]\n",
    "    width, height = img.size\n",
    "    if width > max_width:\n",
    "        max_width = width\n",
    "    if height > max_height:\n",
    "        max_height = height\n",
    "\n",
    "max_size = (max_width, max_height)\n",
    "print(\"Maximum size of all images:\", max_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b339ac1-9a19-4f6f-a513-3f15b26f2572",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "\n",
    "Dividing this subset into training, validation and test sets using a 60/20/20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a59605-c421-4632-b3be-3a26590a9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(data_subset)\n",
    "train_size = int(0.6 * total_size)\n",
    "val_size = int(0.2 * total_size)\n",
    "test_size = total_size - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57246d7c-0c4a-4e9a-b156-a4f3e1f8ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = torch.utils.data.random_split(\n",
    "    data_subset, [train_size, val_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a2a5e-0333-4ff0-9e85-39132468c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Split Size:\\n----------\")\n",
    "print(\"Train dataset size:\", len(train))\n",
    "print(\"Validation dataset size:\", len(val))\n",
    "print(\"Test dataset size:\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f0635-bc80-42f1-90d8-f17bbbe66f65",
   "metadata": {},
   "source": [
    "### Augmenting to 600/200/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d21328-43d5-4a9e-855e-1b9139d66db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomRotation(degrees=15),  # Random rotation up to 10 degrees\n",
    "        transforms.RandomHorizontalFlip(\n",
    "            p=0.5\n",
    "        ),  # Random horizontal flip with a probability of 0.5\n",
    "        transforms.RandomVerticalFlip(\n",
    "            p=0.5\n",
    "        ),  # Random vertical flip with a probability of 0.5\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
    "        ),  # Randomly Adjust brightness, contrast, saturation, and hue\n",
    "        transforms.RandomAffine(\n",
    "            degrees=5, translate=(0.1, 0.1)\n",
    "        ),  # Random affine transformation\n",
    "        transforms.RandomApply(\n",
    "            [transforms.GaussianBlur(kernel_size=3)], p=0.1\n",
    "        ),  # Random Gaussian blur\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e62ef13-24a5-4a8d-bebf-fb2ee7ce3db7",
   "metadata": {},
   "source": [
    "* We're augmenting data after sampling with replacement.\n",
    "* This method increases the chances of seeing more varied augmented versions of the same image.\n",
    "* It's particularly useful when your original dataset is small, as it helps to introduce more variability and potentially prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c923ef-4e41-441c-a790-f0cc3219cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = augment_dataset_with_replacement(train, 600, augmentation_transforms)\n",
    "val_aug = augment_dataset_with_replacement(val, 200, augmentation_transforms)\n",
    "test_aug = augment_dataset_with_replacement(test, 200, augmentation_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78441dd6-573e-49e5-b568-1532d13910e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Augmented train dataset size:\", len(train_aug))\n",
    "print(\"Augmented validation dataset size:\", len(val_aug))\n",
    "print(\"Augmented test dataset size:\", len(test_aug))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17293483-d5ae-40c4-a40d-58f913006d72",
   "metadata": {},
   "source": [
    "### Resizing to 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f601d67f-fda6-4030-8075-55de20bd251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_transform = transforms.Resize((64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab954e-339b-40f5-9d6d-88b20fbd09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug_resized = resize_dataset(train_aug, resize_transform)\n",
    "val_aug_resized = resize_dataset(val_aug, resize_transform)\n",
    "test_aug_resized = resize_dataset(test_aug, resize_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03579b-4b70-440f-a651-525b62cff6f2",
   "metadata": {},
   "source": [
    "### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b7a16-4613-4f43-b56d-90d2b5769efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Subset example:\", train_aug_resized[500][\"text\"])\n",
    "sample_image = train_aug_resized[500][\"image\"]\n",
    "display(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f755966-560c-4b6a-93e1-eb81d2f71c48",
   "metadata": {},
   "source": [
    "### Tensor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f7ad6-855c-45a1-b843-5a0dafd898ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageDataset(train_aug_resized)\n",
    "val_dataset = CustomImageDataset(val_aug_resized)\n",
    "test_dataset = CustomImageDataset(test_aug_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691b402-d41b-48e8-9713-6dd34467e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = float(\"inf\")\n",
    "max_value = float(\"-inf\")\n",
    "\n",
    "for item in train_dataset:\n",
    "    image_tensor = item[\"image\"]\n",
    "\n",
    "    min_value = min(torch.min(image_tensor).item(), min_value)\n",
    "    max_value = max(torch.max(image_tensor).item(), max_value)\n",
    "\n",
    "print(f\"Minimum value across the dataset: {min_value}\")\n",
    "print(f\"Maximum value across the dataset: {max_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f716c163-911d-4c45-a5c1-fea5d0bb6e1b",
   "metadata": {},
   "source": [
    "* This images look to be normalized, so we can use Sigmoid Activation towards the end.\n",
    "* We can also use LeakyReLU as a bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60db9e3-dae6-4c3f-95b7-0e6ecd3aeacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c3f739-1864-493e-a334-fb8432dc8ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a1360-c7a2-4eb0-93e7-932ce4990d84",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Do not run the code below unless you want to perform an extensive grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2516df6-375c-443b-9a13-50c4473ea392",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "learning_rates = [0.001, 0.0001]\n",
    "weight_decays = [1e-5, 1e-4]\n",
    "encoder_channel_options = [\n",
    "    (32, 16, 16),\n",
    "    (64, 32, 32),\n",
    "]  # last element of the tuple is the latent size\n",
    "kernel_sizes = [3]\n",
    "strides = [2]\n",
    "paddings = [1]\n",
    "\n",
    "num_epochs = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b61d5b-5d05-4b23-aeec-338e77532d21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (\n",
    "    encoder_channels,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    kernel_size,\n",
    "    stride,\n",
    "    padding,\n",
    ") in itertools.product(\n",
    "    encoder_channel_options,\n",
    "    learning_rates,\n",
    "    weight_decays,\n",
    "    kernel_sizes,\n",
    "    strides,\n",
    "    paddings,\n",
    "):\n",
    "    latent_size = encoder_channels[-1]\n",
    "    model = AutoEncoder(\n",
    "        latent_size=latent_size,\n",
    "        input_channels=3,\n",
    "        hidden_layer_1=encoder_channels[0],\n",
    "        hidden_layer_2=encoder_channels[1],\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "    )\n",
    "\n",
    "    _, train_losses, val_losses = train_autoencoder(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    # Plot and save losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.title(\n",
    "        f\"LS: {latent_size}, LR: {lr}, WD: {weight_decay}, EC: {encoder_channels}, KS: {kernel_size}, S: {stride}, P: {padding}\"\n",
    "    )\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\n",
    "        \"learning_curves/question_1/\"\n",
    "        + f\"losses_ls{latent_size}_lr{lr}_wd{weight_decay}_ec{encoder_channels}_ks{kernel_size}_s{stride}_p{padding}.png\"\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # Store results\n",
    "    results.append(\n",
    "        {\n",
    "            \"latent_size\": latent_size,\n",
    "            \"learning_rate\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"encoder_channels\": encoder_channels,\n",
    "            \"kernel_size\": kernel_size,\n",
    "            \"stride\": stride,\n",
    "            \"padding\": padding,\n",
    "            \"final_train_loss\": train_losses[-1],\n",
    "            \"final_val_loss\": val_losses[-1],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b2010-c891-462e-a885-1b5e994d8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa499011-8c1c-4a0c-998d-2bb44e2833d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"results/question_1/hyperparam_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da4c48-a854-46ff-b0b5-684812586d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by=\"final_val_loss\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec6a2c-e2cc-44ec-9d4e-3d98598951e0",
   "metadata": {},
   "source": [
    "## Testing Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3512984-5120-4cf2-bf50-49972880f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"results/question_1/hyperparam_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6b859-4b5d-449a-9225-f2711187a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by=\"final_val_loss\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88e93a-49da-400c-a357-9e429eac8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f501b-8092-4c2e-8e0e-e7053d6414ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f7f0a-659b-4fd2-b957-ea66c10c4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.00001\n",
    "encoder_channels = (64, 32, 32)\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "num_epochs = 150\n",
    "latent_size = encoder_channels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab952e92-f470-451f-bbfc-dc77d00ab530",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(\n",
    "    latent_size=latent_size,\n",
    "    hidden_layer_1=encoder_channels[0],\n",
    "    hidden_layer_2=encoder_channels[1],\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    padding=padding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137b399-fab6-4583-8139-e42459028641",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_losses, val_losses = train_autoencoder(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cb976-d266-410a-a5a9-3cc47fe5f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\n",
    "    f\"LS: {latent_size}, LR: {learning_rate}, WD: {weight_decay}, EC: {encoder_channels}, KS: {kernel_size}, S: {stride}, P: {padding}\"\n",
    ")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\n",
    "    \"results/question_1/\"\n",
    "    + f\"losses_ls{latent_size}_lr{learning_rate}_wd{weight_decay}_ec{encoder_channels}_ks{kernel_size}_s{stride}_p{padding}.png\"\n",
    ")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7971ddb8-4f37-4159-a45d-e55c17b9852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "criterion = torch.nn.MSELoss()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch[\"image\"]\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "test_loss /= len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de08a5a0-93b9-4a40-9ea3-996497c5369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4416eaf-d371-4b31-8b87-d3856098271f",
   "metadata": {},
   "source": [
    "## Saving Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403ab1c-1ffb-40ed-8e02-0aa857c6b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture\n",
    "from torchsummary import summary\n",
    "\n",
    "print(summary(model=model, input_size=(3, 64, 64), batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d884ac-e796-4522-99ee-77f64c2905b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 64, 64)\n",
    "output = model(dummy_input)\n",
    "\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.render(\"results/question_1/autoencoder_graph\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97a5d8-213b-4295-baa9-77abccd3e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving weights\n",
    "torch.save(model.state_dict(), \"results/question_1/q1_model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02617e2-9ee2-49ce-b750-86b94e8a8276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent representation\n",
    "model.eval()\n",
    "latent_representations = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch[\"image\"]\n",
    "        latent = model.encoder(images)\n",
    "        latent_representations.append(latent.cpu().numpy())\n",
    "\n",
    "latent_representations = np.concatenate(latent_representations, axis=0)\n",
    "\n",
    "latent_representations_path = \"results/question_1/latent_representations.npy\"\n",
    "np.save(latent_representations_path, latent_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f34656-6d5c-45d2-82d7-498e371261ba",
   "metadata": {},
   "source": [
    "## Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95abc5-e629-4e04-b2dc-c0590ba67265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# take the first image\n",
    "sample_batch = next(iter(test_loader))\n",
    "images = sample_batch[\"image\"]\n",
    "sample_image = images[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed = model(sample_image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9fc6ba-9e0f-4a9a-bd69-67d1692db488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert image\n",
    "sample_image_np = sample_image.numpy().transpose(1, 2, 0)\n",
    "reconstructed_np = reconstructed.squeeze(0).numpy().transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c72d7-e3da-45f4-b671-87ff93d26c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sample_image_np)\n",
    "plt.title(f\"Original Image\")\n",
    "\n",
    "# Reconstructed Image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(reconstructed_np)\n",
    "plt.title(f\"Reconstructed Image\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93622141-fb5a-44ce-a56e-4397f309293a",
   "metadata": {},
   "source": [
    "## Plotting Reconstructed Images vs Original Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7dba8c-ae8a-4763-bd40-ff8eb12e72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_classes = [\n",
    "    \"face with tears of joy\",\n",
    "    \"face palm\",\n",
    "    \"selfie\",\n",
    "]\n",
    "colors = [\"red\", \"green\", \"blue\"]\n",
    "color_map = dict(zip(selected_classes, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d11ef-e239-4cd7-929f-4ee485a0326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_images = []\n",
    "original_images = []\n",
    "filtered_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e881102-4c45-4661-9311-7a1723235125",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images, labels = batch[\"image\"], batch[\"text\"]\n",
    "\n",
    "        mask = [label in selected_classes for label in labels]\n",
    "        if not any(mask):\n",
    "            continue\n",
    "\n",
    "        filtered_images = images[mask]\n",
    "        filtered_labels.extend([labels[i] for i in range(len(labels)) if mask[i]])\n",
    "\n",
    "        # Get the latent representations\n",
    "        latent = model.encoder(filtered_images)\n",
    "\n",
    "        # Reconstruct images from the latent representations\n",
    "        reconstructed = model.decoder(latent)\n",
    "\n",
    "        # Flatten the images for PCA\n",
    "        images_flat = filtered_images.view(filtered_images.size(0), -1)\n",
    "        reconstructed_flat = reconstructed.view(reconstructed.size(0), -1)\n",
    "\n",
    "        original_images.append(images_flat.cpu().numpy())\n",
    "        reconstructed_images.append(reconstructed_flat.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e1806-0750-4cd9-b633-22c937ca49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images_flat = np.concatenate(original_images, axis=0)\n",
    "reconstructed_images_flat = np.concatenate(reconstructed_images, axis=0)\n",
    "filtered_labels = np.array(filtered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71790ac1-c0ca-4144-845f-8a729871c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3d = PCA(n_components=3)\n",
    "reconstructed_3d = pca_3d.fit_transform(reconstructed_images_flat)\n",
    "original_3d = pca_3d.transform(original_images_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1054d-9982-48c6-ad7b-cc544b981165",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    title=\"3D Visualization of Original and Reconstructed Images\",\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Principal Component 1\",\n",
    "        yaxis_title=\"Principal Component 2\",\n",
    "        zaxis_title=\"Principal Component 3\",\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=0),\n",
    ")\n",
    "\n",
    "for class_name in selected_classes:\n",
    "    indices = np.where(filtered_labels == class_name)\n",
    "\n",
    "    # Reconstructed images\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=reconstructed_3d[indices, 0].flatten(),\n",
    "            y=reconstructed_3d[indices, 1].flatten(),\n",
    "            z=reconstructed_3d[indices, 2].flatten(),\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=5, symbol=\"x\", color=color_map[class_name]),\n",
    "            name=f\"{class_name} - Reconstructed\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Original images\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=original_3d[indices, 0].flatten(),\n",
    "            y=original_3d[indices, 1].flatten(),\n",
    "            z=original_3d[indices, 2].flatten(),\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=5, symbol=\"circle\", color=color_map[class_name]),\n",
    "            name=f\"{class_name} - Original\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b473843-2cb1-4be1-b52e-423e07b70703",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "## Composite Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be49a8-9de4-408b-b416-21c04d235133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_reconstructed_image(idx, latent_representations, model):\n",
    "    model.eval()\n",
    "\n",
    "    latent_vector = torch.tensor(latent_representations[idx]).unsqueeze(0).float()\n",
    "\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        latent_vector = latent_vector.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed_img = model.decoder(latent_vector).squeeze(0)\n",
    "\n",
    "    reconstructed_img = reconstructed_img.cpu().numpy()\n",
    "    # Change from CxHxW to HxWxC if needed\n",
    "    reconstructed_img = np.transpose(reconstructed_img, (1, 2, 0))\n",
    "\n",
    "    plt.imshow(reconstructed_img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ec97f-8424-49be-8066-9360e800b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# woman superhero light skin tone\n",
    "display_reconstructed_image(123, latent_representations, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd6d13-c8b2-47d7-b87d-1badae67bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# superhero\n",
    "display_reconstructed_image(48, latent_representations, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3137c6-cab2-4967-86c7-1c9202eaf6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# man getting face massage type 4\n",
    "display_reconstructed_image(105, latent_representations, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65533661-cb2b-4dc3-9c40-358301a3cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# woman superhero light skin tone\n",
    "latent_vector1 = torch.tensor(latent_representations[123]).unsqueeze(0).float()\n",
    "# superhero\n",
    "latent_vector2 = torch.tensor(latent_representations[48]).unsqueeze(0).float()\n",
    "# man getting face massage type 4\n",
    "latent_vector3 = torch.tensor(latent_representations[105]).unsqueeze(0).float()\n",
    "\n",
    "if next(model.parameters()).is_cuda:\n",
    "    latent_vector1 = latent_vector1.cuda()\n",
    "    latent_vector2 = latent_vector2.cuda()\n",
    "    latent_vector3 = latent_vector3.cuda()\n",
    "\n",
    "result_vector = latent_vector1 - latent_vector2 + latent_vector3\n",
    "\n",
    "with torch.no_grad():\n",
    "    new_image = model.decoder(result_vector).squeeze(0)\n",
    "\n",
    "new_image = new_image.cpu().numpy()\n",
    "new_image = np.transpose(new_image, (1, 2, 0))\n",
    "\n",
    "plt.imshow(new_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513910a1-3747-4329-9039-c0081fdb95f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
