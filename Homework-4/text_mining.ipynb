{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1514ecbf-5bbb-4de5-833c-c39b5dbe7694",
   "metadata": {},
   "source": [
    "# Text Mining\n",
    "\n",
    "We're doing a basic EDA to figure out how we're going to reduce our vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac0d02-d907-4c21-ba55-97e3c6bf18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b67f00-1b05-4785-9b3e-8d3b6d38545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f02b5b-baf7-4343-856d-af55fe1d0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/wiki2.train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ba134-b7f6-4801-9b68-a0e6e229db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72169ec-8ba9-43e2-a633-d9e057204682",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter(tokens)\n",
    "freq_dist = pd.DataFrame(token_counts.items(), columns=[\"Token\", \"Frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e64e9b-a686-48a8-a48a-41885a40fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist[\"Token\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a024b0-11f6-4fc7-8e21-32580fdc1e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(freq_dist[\"Frequency\"], bins=50, edgecolor=\"black\")\n",
    "plt.title(\"Histogram of Word Frequencies\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Number of Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe92b56-7f78-4349-9dda-1c2b71117752",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(freq_dist[\"Frequency\"], log_scale=True, bins=50, kde=True, color=\"skyblue\")\n",
    "plt.title(\"Log-Scaled Histogram of Word Frequencies\")\n",
    "plt.xlabel(\"Frequency (log scale)\")\n",
    "plt.ylabel(\"Number of Words\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True, ls=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede3f8a-a2ea-43ea-940c-4bb1dfbc5642",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x=\"Frequency\",\n",
    "    y=\"Token\",\n",
    "    data=freq_dist.sort_values(by=\"Frequency\", ascending=True).head(20),\n",
    ")\n",
    "plt.title(\"Frequency Distribution of Bottom 20 Tokens\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Token\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d120b-e0cf-4d61-88b6-76caaadd1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x=\"Frequency\",\n",
    "    y=\"Token\",\n",
    "    data=freq_dist.sort_values(by=\"Frequency\", ascending=False).head(20),\n",
    ")\n",
    "plt.title(\"Frequency Distribution of Top 20 Tokens\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Token\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce5358-e1b6-4bce-b428-beabe96a62f6",
   "metadata": {},
   "source": [
    "## Pre-processing Documents from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54082368-2916-4e63-b628-1a2d6e653743",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_delimiter_pattern = r\"\\n\\s\\n\\s=\\s[^=]+\\s=\\s\\n\"\n",
    "documents = re.split(document_delimiter_pattern, text)\n",
    "\n",
    "if documents and not documents[0].strip():\n",
    "    documents = documents[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07937aa-774e-4698-a7f7-a6d093642a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = [len(doc.split()) for doc in documents]\n",
    "doc_length_df = pd.DataFrame({\"index\": range(len(doc_lengths)), \"length\": doc_lengths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ebe47-7c52-4ae7-bb60-9fdedd010d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(doc_length_df[\"length\"], bins=50, edgecolor=\"black\")\n",
    "plt.title(\"Histogram of Document Lengths\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Doc Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b1d53-5154-401b-bf27-7e268b3623cb",
   "metadata": {},
   "source": [
    "## Replacing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb5726-3e83-44c5-816a-39fbd6a1f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_stopwords_with_unk(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([\"<unk>\" if token.is_stop else token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eff436-2173-4aac-908e-045d9a8f30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_documents = [replace_stopwords_with_unk(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd5d5a-a73e-411b-a3e0-901e692782bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_documents[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad48a5-9b03-4ec8-a764-af58127f2ddf",
   "metadata": {},
   "source": [
    "## Keeping only English characters and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec74b0e-c70f-4ee0-a1cb-3b85eaba7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_non_alpha_numeric_with_unk(text):\n",
    "    doc = nlp(text)\n",
    "    # Replace tokens that are not alpha (letters) or digit (numbers) with unk\n",
    "    return \" \".join(\n",
    "        [token.text if token.is_alpha or token.is_digit else \"unk\" for token in doc]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e903d-37c9-4185-90da-7c3fc13aa3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_proc_docs = [replace_non_alpha_numeric_with_unk(doc) for doc in processed_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc10a4-879a-47c9-850a-8c9b2b0d530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_proc_docs[10].replace(\"unk \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8871cc99-4dbe-4d41-9586-cf1bba0f1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_doc_lengths = [len(doc.split()) for doc in eng_proc_docs]\n",
    "proc_doc_length_df = pd.DataFrame(\n",
    "    {\"index\": range(len(proc_doc_lengths)), \"length\": proc_doc_lengths}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153f945-e1df-43a0-b2bc-aa807f03f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(proc_doc_length_df[\"length\"], bins=50, edgecolor=\"black\")\n",
    "plt.title(\"Histogram of Processed Document Lengths\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Doc Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437811a3-bca8-4089-8a4f-f6ea73e4be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_combined = \" \".join(eng_proc_docs)\n",
    "all_tokens = all_docs_combined.split()\n",
    "token_counts = Counter(all_tokens)\n",
    "freq_dist = pd.DataFrame(token_counts.items(), columns=[\"Token\", \"Frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6216c-47ec-42e9-8208-5ab5b832f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist[\"Token\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a5021-2678-4e3c-b56b-64590ee48e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "(33277 - 32224) / 33277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480fc2d-1e10-4672-b0e5-6fc191b10b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(\n",
    "    freq_dist.loc[freq_dist[\"Token\"] != \"<unk>\", [\"Frequency\"]],\n",
    "    log_scale=True,\n",
    "    bins=50,\n",
    "    kde=True,\n",
    "    color=\"skyblue\",\n",
    ")\n",
    "plt.title(\"Log-Scaled Histogram of Word Frequencies\")\n",
    "plt.xlabel(\"Frequency (log scale)\")\n",
    "plt.ylabel(\"Number of Words\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True, ls=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0d05e-8f54-4ae3-a1e9-cec7147d08ee",
   "metadata": {},
   "source": [
    "# Replacing based on threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd64f8a-c7e9-4128-8dea-f205c62f5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_proc_docs = [doc.replace(\"unk\", \"<unk>\") for doc in eng_proc_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e94cdd-d10b-4d5c-9c47-d47c3f5f9abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_vocabulary(docs, vocab_size=10000):\n",
    "    # Join the documents\n",
    "    combined_text = \" \".join(docs)\n",
    "\n",
    "    # Tokenize the combined text\n",
    "    all_tokens = combined_text.split()\n",
    "\n",
    "    # Calculate the frequency of each token\n",
    "    token_freq = Counter(all_tokens)\n",
    "\n",
    "    # Identify the tokens to be replaced (those outside the top 'vocab_size' most common)\n",
    "    common_tokens = set(token for token, freq in token_freq.most_common(vocab_size))\n",
    "    tokens_to_replace = set(token for token in token_freq if token not in common_tokens)\n",
    "\n",
    "    # Replace low-frequency tokens with \"<unk>\" and keep track of replaced tokens\n",
    "    replaced_tokens = set()\n",
    "    reduced_docs = []\n",
    "    for doc in docs:\n",
    "        new_tokens = []\n",
    "        for token in doc.split():\n",
    "            if token in tokens_to_replace:\n",
    "                new_tokens.append(\"<unk>\")\n",
    "                replaced_tokens.add(token)\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        reduced_docs.append(\" \".join(new_tokens))\n",
    "\n",
    "    return reduced_docs, replaced_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe5b9a-9f53-4253-9ab6-64e95379a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_docs, replaced_tokens = reduce_vocabulary(eng_proc_docs, vocab_size=12500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322bd0d-7600-4508-b9eb-e9e36bad6b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokens_to_file(tokens, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for token in tokens:\n",
    "            file.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd57dc-6067-45c0-93ca-1856ec4981f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tokens_to_file(replaced_tokens, \"data/replaced_tokens.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761fdfd-187a-4d44-be40-1433ea744779",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_docs[10].replace(\"<unk> \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00ec86-0d1d-4f79-87e6-bb792f787925",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_combined = \" \".join(eng_proc_docs)\n",
    "all_tokens = all_docs_combined.split()\n",
    "token_counts = Counter(all_tokens)\n",
    "freq_dist = pd.DataFrame(token_counts.items(), columns=[\"Token\", \"Frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8dde00-aaee-4cd4-962a-97972f4feb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(\n",
    "    freq_dist.loc[freq_dist[\"Token\"] != \"<unk>\", [\"Frequency\"]],\n",
    "    log_scale=True,\n",
    "    bins=50,\n",
    "    kde=True,\n",
    "    color=\"skyblue\",\n",
    ")\n",
    "plt.title(\"Log-Scaled Histogram of Word Frequencies\")\n",
    "plt.xlabel(\"Frequency (log scale)\")\n",
    "plt.ylabel(\"Number of Words\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True, ls=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be59fd2-8503-4b38-8908-98b93c3e734c",
   "metadata": {},
   "source": [
    "## Saving the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ead82c-2d53-4685-b935-fa0c1d37916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_saver import DataSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dca954-5e10-471b-a346-29a4de2b2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_saver = DataSaver(\n",
    "#     input_file=\"data/wiki2.train.txt\",\n",
    "#     tokens_to_replace_file=\"data/replaced_tokens.txt\",\n",
    "#     output_file=\"data/wiki2.train_processed.txt\",\n",
    "# )\n",
    "# train_data_saver.save_processed_data()\n",
    "\n",
    "# test_data_saver = DataSaver(\n",
    "#     input_file=\"data/wiki2.test.txt\",\n",
    "#     tokens_to_replace_file=\"data/replaced_tokens.txt\",\n",
    "#     output_file=\"data/wiki2.test_processed.txt\",\n",
    "# )\n",
    "# test_data_saver.save_processed_data()\n",
    "\n",
    "# valid_data_saver = DataSaver(\n",
    "#     input_file=\"data/wiki2.valid.txt\",\n",
    "#     tokens_to_replace_file=\"data/replaced_tokens.txt\",\n",
    "#     output_file=\"data/wiki2.valid_processed.txt\",\n",
    "# )\n",
    "# valid_data_saver.save_processed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e1148-b472-43bf-8d97-217afa14b66c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
